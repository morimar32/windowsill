{
  "domain": "data_science",
  "timestamp": "2026-02-21T08:13:19.654800+00:00",
  "n_words": 59,
  "core": [
    "XGBoost",
    "anomaly detection",
    "bagging",
    "boosting",
    "classification",
    "clustering",
    "confusion matrix",
    "cross-validation",
    "decision tree",
    "deep learning",
    "dimensionality reduction",
    "ensemble learning",
    "feature engineering",
    "feature selection",
    "gradient descent",
    "hyperparameter tuning",
    "k-means",
    "logistic regression",
    "neural network",
    "overfitting",
    "pandas",
    "precision-recall",
    "random forest",
    "regularization",
    "scikit-learn",
    "supervised learning",
    "tensorflow",
    "train-test split",
    "underfitting",
    "unsupervised learning"
  ],
  "peripheral": [
    "array broadcasting",
    "baseline model",
    "batch normalization",
    "bias-variance tradeoff",
    "categorical encoding",
    "convolutional layer",
    "correlation matrix",
    "data augmentation",
    "data wrangling",
    "dropout layer",
    "embedding",
    "exploratory analysis",
    "gradient boosting",
    "imbalanced dataset",
    "imputation",
    "jupyter notebook",
    "kernel trick",
    "learning curve",
    "loss function",
    "matplotlib",
    "missing values",
    "normalized data",
    "numpy",
    "outlier removal",
    "pipeline",
    "recurrent network",
    "scaling",
    "seaborn",
    "validation set"
  ],
  "words": [
    "XGBoost",
    "anomaly detection",
    "array broadcasting",
    "bagging",
    "baseline model",
    "batch normalization",
    "bias-variance tradeoff",
    "boosting",
    "categorical encoding",
    "classification",
    "clustering",
    "confusion matrix",
    "convolutional layer",
    "correlation matrix",
    "cross-validation",
    "data augmentation",
    "data wrangling",
    "decision tree",
    "deep learning",
    "dimensionality reduction",
    "dropout layer",
    "embedding",
    "ensemble learning",
    "exploratory analysis",
    "feature engineering",
    "feature selection",
    "gradient boosting",
    "gradient descent",
    "hyperparameter tuning",
    "imbalanced dataset",
    "imputation",
    "jupyter notebook",
    "k-means",
    "kernel trick",
    "learning curve",
    "logistic regression",
    "loss function",
    "matplotlib",
    "missing values",
    "neural network",
    "normalized data",
    "numpy",
    "outlier removal",
    "overfitting",
    "pandas",
    "pipeline",
    "precision-recall",
    "random forest",
    "recurrent network",
    "regularization",
    "scaling",
    "scikit-learn",
    "seaborn",
    "supervised learning",
    "tensorflow",
    "train-test split",
    "underfitting",
    "unsupervised learning",
    "validation set"
  ]
}